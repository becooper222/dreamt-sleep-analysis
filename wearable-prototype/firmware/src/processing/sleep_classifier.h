/**
 * Sleep Stage Classifier - TFLite Micro Inference
 * ================================================
 * 
 * Runs the trained MLP model on ESP32 to classify sleep stages
 * from extracted IMU + PPG features.
 * 
 * Classes:
 *   0: Wake
 *   1: Light Sleep (N1 + N2)
 *   2: Deep Sleep (N3)
 *   3: REM
 */

#ifndef SLEEP_CLASSIFIER_H
#define SLEEP_CLASSIFIER_H

#include <Arduino.h>
#include "tensorflow/lite/micro/micro_interpreter.h"
#include "tensorflow/lite/micro/micro_mutable_op_resolver.h"
#include "tensorflow/lite/schema/schema_generated.h"
#include "feature_extractor.h"

// Include the model data (generated from TFLite model)
// This will be created by: xxd -i sleep_model.tflite > model_data.h
#include "model_data.h"

// Include scaler parameters (generated by training script)
#include "scaler_params.h"

// ============================================================================
// Configuration
// ============================================================================

// Memory arena for TFLite (adjust based on model size)
#define TENSOR_ARENA_SIZE   (32 * 1024)  // 32 KB

// Number of output classes
#define N_SLEEP_CLASSES     4

// Class names
const char* SLEEP_CLASS_NAMES[N_SLEEP_CLASSES] = {
    "Wake", "Light", "Deep", "REM"
};


// ============================================================================
// Sleep Stage Result
// ============================================================================

struct SleepStageResult {
    uint8_t predictedClass;          // 0-3
    const char* className;           // "Wake", "Light", "Deep", "REM"
    float confidence;                // Highest probability
    float probabilities[N_SLEEP_CLASSES];  // All class probabilities
    uint32_t timestamp;
    bool valid;
    float inferenceTimeMs;
};


// ============================================================================
// Sleep Classifier Class
// ============================================================================

class SleepClassifier {
public:
    SleepClassifier() : _initialized(false), _interpreter(nullptr) {}
    
    /**
     * Initialize the TFLite interpreter.
     * 
     * @return true if initialization successful
     */
    bool begin() {
        Serial.println("[TFLITE] Initializing sleep classifier...");
        
        // Allocate tensor arena
        _tensorArena = (uint8_t*)malloc(TENSOR_ARENA_SIZE);
        if (!_tensorArena) {
            Serial.println("[TFLITE] Failed to allocate tensor arena!");
            return false;
        }
        
        // Load the model
        _model = tflite::GetModel(sleep_model_tflite);
        if (_model->version() != TFLITE_SCHEMA_VERSION) {
            Serial.printf("[TFLITE] Model schema version mismatch: %d vs %d\n",
                         _model->version(), TFLITE_SCHEMA_VERSION);
            return false;
        }
        
        // Set up the op resolver (add only the ops your model needs)
        // MLP typically needs: FullyConnected, Relu, Softmax, BatchNorm (if used)
        static tflite::MicroMutableOpResolver<10> resolver;
        resolver.AddFullyConnected();
        resolver.AddRelu();
        resolver.AddSoftmax();
        resolver.AddReshape();
        resolver.AddQuantize();
        resolver.AddDequantize();
        // Add BatchNorm ops if your model uses them
        // resolver.AddMul();
        // resolver.AddAdd();
        
        // Build the interpreter
        static tflite::MicroInterpreter static_interpreter(
            _model, resolver, _tensorArena, TENSOR_ARENA_SIZE);
        _interpreter = &static_interpreter;
        
        // Allocate tensors
        TfLiteStatus allocate_status = _interpreter->AllocateTensors();
        if (allocate_status != kTfLiteOk) {
            Serial.println("[TFLITE] AllocateTensors() failed!");
            return false;
        }
        
        // Get input/output tensor info
        _input = _interpreter->input(0);
        _output = _interpreter->output(0);
        
        Serial.printf("[TFLITE] Input: dims=%d, type=%d\n", 
                     _input->dims->size, _input->type);
        Serial.printf("[TFLITE] Output: dims=%d, type=%d\n",
                     _output->dims->size, _output->type);
        Serial.printf("[TFLITE] Arena used: %d bytes\n",
                     _interpreter->arena_used_bytes());
        
        _initialized = true;
        Serial.println("[TFLITE] Classifier ready!");
        
        return true;
    }
    
    /**
     * Classify sleep stage from extracted features.
     * 
     * @param features Extracted epoch features
     * @param result Output classification result
     * @return true if classification successful
     */
    bool classify(const EpochFeatures& features, SleepStageResult& result) {
        if (!_initialized) {
            result.valid = false;
            return false;
        }
        
        if (!features.valid) {
            result.valid = false;
            return false;
        }
        
        unsigned long startTime = micros();
        
        // Scale features using the scaler parameters from training
        float scaledFeatures[N_FEATURES];
        for (int i = 0; i < N_FEATURES; i++) {
            scaledFeatures[i] = (features.features[i] - FEATURE_MEAN[i]) / FEATURE_SCALE[i];
        }
        
        // Copy scaled features to input tensor
        if (_input->type == kTfLiteFloat32) {
            // Float model
            float* inputData = _input->data.f;
            for (int i = 0; i < N_FEATURES; i++) {
                inputData[i] = scaledFeatures[i];
            }
        } else if (_input->type == kTfLiteInt8) {
            // Quantized model
            int8_t* inputData = _input->data.int8;
            float scale = _input->params.scale;
            int zero_point = _input->params.zero_point;
            
            for (int i = 0; i < N_FEATURES; i++) {
                int32_t quantized = (int32_t)(scaledFeatures[i] / scale) + zero_point;
                quantized = max(-128, min(127, quantized));
                inputData[i] = (int8_t)quantized;
            }
        }
        
        // Run inference
        TfLiteStatus invoke_status = _interpreter->Invoke();
        if (invoke_status != kTfLiteOk) {
            Serial.println("[TFLITE] Invoke failed!");
            result.valid = false;
            return false;
        }
        
        // Extract output probabilities
        if (_output->type == kTfLiteFloat32) {
            float* outputData = _output->data.f;
            for (int i = 0; i < N_SLEEP_CLASSES; i++) {
                result.probabilities[i] = outputData[i];
            }
        } else if (_output->type == kTfLiteInt8) {
            int8_t* outputData = _output->data.int8;
            float scale = _output->params.scale;
            int zero_point = _output->params.zero_point;
            
            for (int i = 0; i < N_SLEEP_CLASSES; i++) {
                result.probabilities[i] = (outputData[i] - zero_point) * scale;
            }
        }
        
        // Find predicted class (argmax)
        uint8_t maxClass = 0;
        float maxProb = result.probabilities[0];
        for (int i = 1; i < N_SLEEP_CLASSES; i++) {
            if (result.probabilities[i] > maxProb) {
                maxProb = result.probabilities[i];
                maxClass = i;
            }
        }
        
        // Fill result
        result.predictedClass = maxClass;
        result.className = SLEEP_CLASS_NAMES[maxClass];
        result.confidence = maxProb;
        result.timestamp = millis();
        result.valid = true;
        result.inferenceTimeMs = (micros() - startTime) / 1000.0f;
        
        return true;
    }
    
    /**
     * Get classifier status.
     */
    bool isReady() const {
        return _initialized;
    }
    
    /**
     * Get memory usage.
     */
    size_t getArenaUsed() const {
        return _initialized ? _interpreter->arena_used_bytes() : 0;
    }
    
private:
    bool _initialized;
    const tflite::Model* _model;
    tflite::MicroInterpreter* _interpreter;
    TfLiteTensor* _input;
    TfLiteTensor* _output;
    uint8_t* _tensorArena;
};

#endif // SLEEP_CLASSIFIER_H

